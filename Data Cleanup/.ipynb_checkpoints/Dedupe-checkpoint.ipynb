{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dedupe file is meant to deduplicate single files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to edit this before running\n",
    "input_file = '../Data/'\n",
    "output_file = '../Data/dedupe_results/sunshine_physicians/sunshine_physicians.csv'\n",
    "settings_file = '../Data/dedupe_results/sunshine_physicians_learned_settings'\n",
    "training_file = '../Data/dedupe_results/sunshine_physicians_training.json'\n",
    "id_column = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [{'field':'price','type':'Price','has missing': True},\n",
    "                  {'field':'modelno','type':'String','has missing': True},\n",
    "                  {'field':'title','type':'Text','has missing': True,'corpus': descriptions()},\n",
    "                  {'field':'brand','type':'String','has missing': True}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])\n",
    "            row_id = int(row[id_column])\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:Predicate set:\n",
      "INFO:dedupe.api:(TfidfTextSearchPredicate: (0.6, title), SimplePredicate: (commonThreeTokens, title))\n",
      "INFO:dedupe.api:(LevenshteinSearchPredicate: (1, modelno), SimplePredicate: (orderOfMagnitude, price), SimplePredicate: (sameSevenCharStartPredicate, brand))\n",
      "INFO:dedupe.api:(TfidfTextSearchPredicate: (0.4, title), SimplePredicate: (roundTo1, price))\n",
      "INFO:dedupe.api:(TfidfTextSearchPredicate: (0.6, title), SimplePredicate: (commonTwoTokens, brand), SimplePredicate: (hundredIntegersOddPredicate, title))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from ./runs/walmart_amazon_learned_settings\n",
      "clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.blocking:10000, 2.7190802 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# duplicate sets 836\n",
      "writing file\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print('importing data ...')\n",
    "    data_d = readData(input_file)    \n",
    "       \n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from', settings_file)\n",
    "        with open(settings_file, 'rb') as f:\n",
    "            deduper = dedupe.StaticDedupe(f)\n",
    "    \n",
    "    else:        \n",
    "        deduper = dedupe.Dedupe(fields)\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                deduper.prepare_training(data_d,\n",
    "                                        training_file=tf,\n",
    "                                        sample_size=15000)\n",
    "        else:\n",
    "            deduper.prepare_training(data_d)\n",
    "            \n",
    "        print('starting active labeling...')\n",
    "        dedupe.console_label(deduper)\n",
    "        deduper.train()\n",
    "        with open(training_file, 'w') as tf:\n",
    "            deduper.write_training(tf)\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "    \n",
    "    print('clustering...')    \n",
    "    clustered_dupes = deduper.partition(data_d, 0.5)\n",
    "    print('# duplicate sets', len(clustered_dupes))\n",
    "    \n",
    "    cluster_membership = {}\n",
    "    for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "        for record_id, score in zip(records, scores):\n",
    "            cluster_membership[record_id] = {\n",
    "                \"Cluster ID\": cluster_id,\n",
    "                \"confidence_score\": score\n",
    "            }\n",
    "    print('writing file')\n",
    "    with open(output_file, 'w') as f_output, open(input_file) as f_input:\n",
    "\n",
    "        reader = csv.DictReader(f_input)\n",
    "        fieldnames = ['Cluster ID', 'confidence_score'] + reader.fieldnames\n",
    "\n",
    "        writer = csv.DictWriter(f_output, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            row_id = int(row['Id'])\n",
    "            row.update(cluster_membership[row_id])\n",
    "            writer.writerow(row)\n",
    "        print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

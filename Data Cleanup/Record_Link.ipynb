{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dedupe file is meant to deduplicate single files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to edit this before running\n",
    "input_file_left = '../Data/Outputs_Cleanup/physicians_info_dedup.csv'\n",
    "input_file_right = '../Data/Outputs_Cleanup/prescriber_dedup.csv'\n",
    "output_file = '../Data/recordLink_results/physicians.csv'\n",
    "settings_file = '../Data/recordLink_results/physicians_learned_settings'\n",
    "training_file = '../Data/recordLink_results/physicians_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [{'field':'fname','type':'String'},\n",
    "                  {'field':'lname','type':'String'},\n",
    "                  {'field':'type','type':'String'},\n",
    "                  {'field':'city','type':'String'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])\n",
    "            data_d[filename + str(i)] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print('importing data ...')\n",
    "    data_1 = readData(input_file_left)\n",
    "    data_2 = readData(input_file_right)   \n",
    "       \n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            linker = dedupe.StaticRecordLink(sf)\n",
    "    \n",
    "    else:        \n",
    "        linker = dedupe.RecordLink(fields)\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                linker.prepare_training(data_1,\n",
    "                                        data_2,\n",
    "                                        training_file=tf,\n",
    "                                        sample_size=15000)\n",
    "        else:\n",
    "            linker.prepare_training(data_1, data_2, sample_size=5000,blocked_proportion=0.5)\n",
    "        print('starting active labeling...')\n",
    "        dedupe.console_label(linker)\n",
    "        linker.train()\n",
    "        with open(training_file, 'w') as tf:\n",
    "            linker.write_training(tf)\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            linker.write_settings(sf)\n",
    "    \n",
    "    print('clustering...')    \n",
    "    linked_records = linker.join(data_1, data_2, threshold=0.5,constraint='one-to-one')\n",
    "    print('# duplicate sets', len(linked_records))\n",
    "    \n",
    "    cluster_membership = {}\n",
    "    for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "        for record_id in cluster:\n",
    "            cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
    "                                             'Link Score': score}\n",
    "    print('writing file')\n",
    "    with open(output_file, 'w',newline='') as f:\n",
    "\n",
    "        header_unwritten = True\n",
    "\n",
    "        for fileno, filename in enumerate((left_file, right_file)):\n",
    "            with open(filename) as f_input:\n",
    "                reader = csv.DictReader(f_input)\n",
    "\n",
    "                if header_unwritten:\n",
    "\n",
    "                    fieldnames = (['Cluster ID', 'Link Score', 'source file'] +\n",
    "                                  reader.fieldnames)\n",
    "\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                    header_unwritten = False\n",
    "\n",
    "                for row_id, row in enumerate(reader):\n",
    "\n",
    "                    record_id = filename + str(row_id)\n",
    "                    cluster_details = cluster_membership.get(record_id, {})\n",
    "                    row['source file'] = fileno\n",
    "                    row.update(cluster_details)\n",
    "\n",
    "                    writer.writerow(row)\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

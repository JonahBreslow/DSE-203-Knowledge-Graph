{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dedupe file is meant to deduplicate single files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to edit this before running\n",
    "input_file_left = '../Data/Outputs_Cleanup/physicians_info_dedup.csv'\n",
    "input_file_right = '../Data/Outputs_Cleanup/prescriber_dedup.csv'\n",
    "output_file = '../Data/recordLink_results/physicians.csv'\n",
    "settings_file = '../Data/recordLink_results/physicians_learned_settings'\n",
    "training_file = '../Data/recordLink_results/physicians_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [{'field':'fname','type':'String'},\n",
    "                  {'field':'lname','type':'String'},\n",
    "                  {'field':'type','type':'String'},\n",
    "                  {'field':'city','type':'String'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])\n",
    "            data_d[filename + str(i)] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-430d76e26032>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                         sample_size=15000)\n\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mlinker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblocked_proportion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'starting active labeling...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdedupe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsole_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dedupe\\api.py\u001b[0m in \u001b[0;36mprepare_training\u001b[1;34m(self, data_1, data_2, training_file, sample_size, blocked_proportion, original_length_1, original_length_2)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraining_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m         self._sample(data_1,\n\u001b[0m\u001b[0;32m   1350\u001b[0m                      \u001b[0mdata_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m                      \u001b[0msample_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dedupe\\api.py\u001b[0m in \u001b[0;36m_sample\u001b[1;34m(self, data_1, data_2, sample_size, blocked_proportion, original_length_1, original_length_2)\u001b[0m\n\u001b[0;32m   1379\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1381\u001b[1;33m         self.active_learner = self.ActiveLearner(self.data_model,\n\u001b[0m\u001b[0;32m   1382\u001b[0m                                                  \u001b[0mdata_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m                                                  \u001b[0mdata_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dedupe\\labeler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_model, data_1, data_2, blocked_proportion, sample_size, original_length_1, original_length_2, index_include)\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[0mdata_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         self.candidates = self._sample(data_1,\n\u001b[0m\u001b[0;32m    467\u001b[0m                                        \u001b[0mdata_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m                                        \u001b[0mblocked_proportion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dedupe\\labeler.py\u001b[0m in \u001b[0;36m_sample\u001b[1;34m(self, data_1, data_2, blocked_proportion, sample_size)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mrandom_sample_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocked_sample_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         random_sample_keys = core.randomPairsMatch(len(deque_1),\n\u001b[0m\u001b[0;32m     82\u001b[0m                                                    \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeque_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                                                    random_sample_size)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dedupe\\core.py\u001b[0m in \u001b[0;36mrandomPairsMatch\u001b[1;34m(n_records_A, n_records_B, sample_size)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mrandom_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         random_pairs = numpy.array(random.sample(range(n), sample_size),\n\u001b[0m\u001b[0;32m     85\u001b[0m                                    dtype=int)\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    print('importing data ...')\n",
    "    data_1 = readData(input_file_left)\n",
    "    data_2 = readData(input_file_right)   \n",
    "       \n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            linker = dedupe.StaticRecordLink(sf)\n",
    "    \n",
    "    else:        \n",
    "        linker = dedupe.RecordLink(fields)\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                linker.prepare_training(data_1,\n",
    "                                        data_2,\n",
    "                                        training_file=tf,\n",
    "                                        sample_size=15000)\n",
    "        else:\n",
    "            linker.prepare_training(data_1, data_2, sample_size=5000,blocked_proportion=0.5)\n",
    "        print('starting active labeling...')\n",
    "        dedupe.console_label(linker)\n",
    "        linker.train()\n",
    "        with open(training_file, 'w') as tf:\n",
    "            linker.write_training(tf)\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            linker.write_settings(sf)\n",
    "    \n",
    "    print('clustering...')    \n",
    "    linked_records = linker.join(data_1, data_2, threshold=0.5,constraint='one-to-one')\n",
    "    print('# duplicate sets', len(linked_records))\n",
    "    \n",
    "    cluster_membership = {}\n",
    "    for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "        for record_id in cluster:\n",
    "            cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
    "                                             'Link Score': score}\n",
    "    print('writing file')\n",
    "    with open(output_file, 'w',newline='') as f:\n",
    "\n",
    "        header_unwritten = True\n",
    "\n",
    "        for fileno, filename in enumerate((left_file, right_file)):\n",
    "            with open(filename) as f_input:\n",
    "                reader = csv.DictReader(f_input)\n",
    "\n",
    "                if header_unwritten:\n",
    "\n",
    "                    fieldnames = (['Cluster ID', 'Link Score', 'source file'] +\n",
    "                                  reader.fieldnames)\n",
    "\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                    header_unwritten = False\n",
    "\n",
    "                for row_id, row in enumerate(reader):\n",
    "\n",
    "                    record_id = filename + str(row_id)\n",
    "                    cluster_details = cluster_membership.get(record_id, {})\n",
    "                    row['source file'] = fileno\n",
    "                    row.update(cluster_details)\n",
    "\n",
    "                    writer.writerow(row)\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

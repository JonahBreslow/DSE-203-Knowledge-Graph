{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146674af",
   "metadata": {
    "papermill": {
     "duration": 0.033035,
     "end_time": "2021-12-08T18:41:02.707426",
     "exception": false,
     "start_time": "2021-12-08T18:41:02.674391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Matching Sunshine Act Drug Data to Pre-Matched OpenFDA - Medicare Part D Drug Data\n",
    "## Authors: \n",
    "    1. Lam Ho\n",
    "    2. Jonah Breslow\n",
    "    3. Jeffrey Kagan\n",
    "## Purpose:\n",
    "The purpose of this notebook is to match the drug data from the Sunshine Act Data to the pre-matched Medicare Part D - OpenFDA Drug data. For\n",
    "this procedure, we utilized the Dedupe.io python implementation.\n",
    "\n",
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7a9d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T18:41:02.734383Z",
     "iopub.status.busy": "2021-12-08T18:41:02.734383Z",
     "iopub.status.idle": "2021-12-08T18:41:03.862937Z",
     "shell.execute_reply": "2021-12-08T18:41:03.863943Z"
    },
    "papermill": {
     "duration": 1.143521,
     "end_time": "2021-12-08T18:41:03.863943",
     "exception": false,
     "start_time": "2021-12-08T18:41:02.720422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This code demonstrates how to use RecordLink with two comma separated\n",
    "values (CSV) files. We have listings of products from two different\n",
    "online stores. The task is to link products between the datasets.\n",
    "\n",
    "The output will be a CSV with our linkded results.\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import optparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da61a8e6-6734-482f-8d32-1f9a77f57120",
   "metadata": {
    "papermill": {
     "duration": 0.010962,
     "end_time": "2021-12-08T18:41:03.884934",
     "exception": false,
     "start_time": "2021-12-08T18:41:03.873972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede09014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T18:41:03.914935Z",
     "iopub.status.busy": "2021-12-08T18:41:03.914935Z",
     "iopub.status.idle": "2021-12-08T18:41:03.957936Z",
     "shell.execute_reply": "2021-12-08T18:41:03.956936Z"
    },
    "papermill": {
     "duration": 0.059001,
     "end_time": "2021-12-08T18:41:03.957936",
     "exception": false,
     "start_time": "2021-12-08T18:41:03.898935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fda_partD_drugs = pd.read_csv('Data/Outputs_Cleanup/FDA_partD_drug_matching/fda_partD_drugs_matched.csv')\n",
    "fda_partD_drugs = pickle.load(open(\"Data/Outputs_Cleanup/FDA_partD_drug_matching/fda_partD_drugs_matched.p\", \"rb\" )).reset_index()\n",
    "df = fda_partD_drugs[['brand_name']]\n",
    "df.to_csv('Data/Outputs_Cleanup/Sunshine_dedupe_drug_integration/fda_partD_input.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4772a-ba6a-4b1d-a09e-71f3d5531b33",
   "metadata": {
    "papermill": {
     "duration": 0.008959,
     "end_time": "2021-12-08T18:41:03.977935",
     "exception": false,
     "start_time": "2021-12-08T18:41:03.968976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Running the Dedupe procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72a5720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T18:41:04.003935Z",
     "iopub.status.busy": "2021-12-08T18:41:04.003935Z",
     "iopub.status.idle": "2021-12-08T18:41:04.019936Z",
     "shell.execute_reply": "2021-12-08T18:41:04.019936Z"
    },
    "papermill": {
     "duration": 0.031967,
     "end_time": "2021-12-08T18:41:04.019936",
     "exception": false,
     "start_time": "2021-12-08T18:41:03.987969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "\n",
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID.\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])\n",
    "            data_d[filename + str(i)] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d53e71-633b-4e25-a792-4cc1b78c88d1",
   "metadata": {
    "papermill": {
     "duration": 0.008989,
     "end_time": "2021-12-08T18:41:04.038939",
     "exception": false,
     "start_time": "2021-12-08T18:41:04.029950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e474a201-7e50-4d1e-8b83-c0f24dc67334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T18:41:04.060976Z",
     "iopub.status.busy": "2021-12-08T18:41:04.059936Z",
     "iopub.status.idle": "2021-12-08T18:41:04.066938Z",
     "shell.execute_reply": "2021-12-08T18:41:04.067979Z"
    },
    "papermill": {
     "duration": 0.019044,
     "end_time": "2021-12-08T18:41:04.067979",
     "exception": false,
     "start_time": "2021-12-08T18:41:04.048935",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "retrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050a3181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T18:41:04.087938Z",
     "iopub.status.busy": "2021-12-08T18:41:04.087938Z",
     "iopub.status.idle": "2021-12-08T18:41:04.097938Z",
     "shell.execute_reply": "2021-12-08T18:41:04.097938Z"
    },
    "papermill": {
     "duration": 0.021944,
     "end_time": "2021-12-08T18:41:04.097938",
     "exception": false,
     "start_time": "2021-12-08T18:41:04.075994",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "retrain = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e9cce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T18:41:04.132942Z",
     "iopub.status.busy": "2021-12-08T18:41:04.127941Z",
     "iopub.status.idle": "2021-12-08T18:41:13.152080Z",
     "shell.execute_reply": "2021-12-08T18:41:13.152080Z"
    },
    "papermill": {
     "duration": 9.043132,
     "end_time": "2021-12-08T18:41:13.153076",
     "exception": false,
     "start_time": "2021-12-08T18:41:04.109944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:Predicate set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:(TfidfNGramSearchPredicate: (0.6, brand_name), SimplePredicate: (sameThreeCharStartPredicate, brand_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n",
      "reading from Data/Outputs_Cleanup/Sunshine_dedupe_drug_integration/data_matching_learned_settings\n",
      "clustering...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# duplicate sets 33\n"
     ]
    }
   ],
   "source": [
    "if retrain == True:\n",
    "    try:\n",
    "        os.remove('csv_example_learned_settings')\n",
    "    except:\n",
    "        print('Your settings file appears to not have existed.')\n",
    "    \n",
    "    \n",
    "output_file = 'Data/Outputs_Cleanup/Sunshine_dedupe_drug_integration/data_matching_output.csv'\n",
    "settings_file = 'Data/Outputs_Cleanup/Sunshine_dedupe_drug_integration/data_matching_learned_settings'\n",
    "training_file = 'Data/Outputs_Cleanup/Sunshine_dedupe_drug_integration/data_matching_training.json'\n",
    "\n",
    "left_file = 'Data/Outputs_Cleanup/Sunshine_dedupe_drug_integration/fda_partD_input.csv'\n",
    "right_file = 'Data/Outputs_Cleanup/Sunshine/sunshine_drugs_distinct.csv'\n",
    "\n",
    "print('importing data ...')\n",
    "data_1 = readData(left_file)\n",
    "data_2 = readData(right_file)\n",
    "\n",
    "def descriptions():\n",
    "    for dataset in (data_1, data_2):\n",
    "        for record in dataset.values():\n",
    "            yield record['description']\n",
    "\n",
    "# ## Training\n",
    "\n",
    "if os.path.exists(settings_file):\n",
    "    print('reading from', settings_file)\n",
    "    with open(settings_file, 'rb') as sf:\n",
    "        linker = dedupe.StaticRecordLink(sf)\n",
    "\n",
    "else:\n",
    "    # Define the fields the linker will pay attention to\n",
    "    #\n",
    "    # Notice how we are telling the linker to use a custom field comparator\n",
    "    # for the 'price' field.\n",
    "    fields = [\n",
    "        {'field': 'brand_name', 'type': 'String'},\n",
    "        #{'field': 'title', 'type': 'Text', 'corpus': descriptions()},\n",
    "        #{'field': 'description', 'type': 'Text',\n",
    "        # 'has missing': True, 'corpus': descriptions()},\n",
    "        #{'field': 'price', 'type': 'Price', 'has missing': True}\n",
    "    ]\n",
    "\n",
    "    # Create a new linker object and pass our data model to it.\n",
    "    linker = dedupe.RecordLink(fields)\n",
    "\n",
    "    # If we have training data saved from a previous run of linker,\n",
    "    # look for it an load it in.\n",
    "    # __Note:__ if you want to train from scratch, delete the training_file\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file) as tf:\n",
    "            linker.prepare_training(data_1,\n",
    "                                    data_2,\n",
    "                                    training_file=tf,\n",
    "                                    sample_size=15000)\n",
    "    else:\n",
    "        linker.prepare_training(data_1, data_2, sample_size=15000)\n",
    "\n",
    "    # ## Active learning\n",
    "    # Dedupe will find the next pair of records\n",
    "    # it is least certain about and ask you to label them as matches\n",
    "    # or not.\n",
    "    # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "    # press 'f' when you are finished\n",
    "    print('starting active labeling...')\n",
    "\n",
    "    dedupe.console_label(linker)\n",
    "\n",
    "    linker.train()\n",
    "\n",
    "    # When finished, save our training away to disk\n",
    "    with open(training_file, 'w') as tf:\n",
    "        linker.write_training(tf)\n",
    "\n",
    "    # Save our weights and predicates to disk.  If the settings file\n",
    "    # exists, we will skip all the training and learning next time we run\n",
    "    # this file.\n",
    "    with open(settings_file, 'wb') as sf:\n",
    "        linker.write_settings(sf)\n",
    "\n",
    "# ## Blocking\n",
    "\n",
    "# ## Clustering\n",
    "\n",
    "# Find the threshold that will maximize a weighted average of our\n",
    "# precision and recall.  When we set the recall weight to 2, we are\n",
    "# saying we care twice as much about recall as we do precision.\n",
    "#\n",
    "# If we had more data, we would not pass in all the blocked data into\n",
    "# this function but a representative sample.\n",
    "\n",
    "print('clustering...')\n",
    "linked_records = linker.join(data_1, data_2, 0.0)\n",
    "\n",
    "print('# duplicate sets', len(linked_records))\n",
    "# ## Writing Results\n",
    "\n",
    "# Write our original data back out to a CSV with a new column called\n",
    "# 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "cluster_membership = {}\n",
    "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
    "                                         'Link Score': score}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "\n",
    "    header_unwritten = True\n",
    "\n",
    "    for fileno, filename in enumerate((left_file, right_file)):\n",
    "        with open(filename) as f_input:\n",
    "            reader = csv.DictReader(f_input)\n",
    "\n",
    "            if header_unwritten:\n",
    "\n",
    "                fieldnames = (['Cluster ID', 'Link Score', 'source file'] +\n",
    "                              reader.fieldnames)\n",
    "\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "\n",
    "                header_unwritten = False\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "\n",
    "                record_id = filename + str(row_id)\n",
    "                cluster_details = cluster_membership.get(record_id, {})\n",
    "                row['source file'] = fileno\n",
    "                row.update(cluster_details)\n",
    "\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553f5a3-249c-425b-8294-635eb6de9de4",
   "metadata": {
    "papermill": {
     "duration": 0.010001,
     "end_time": "2021-12-08T18:41:13.173084",
     "exception": false,
     "start_time": "2021-12-08T18:41:13.163083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0151962d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T18:41:13.197035Z",
     "iopub.status.busy": "2021-12-08T18:41:13.197035Z",
     "iopub.status.idle": "2021-12-08T18:41:13.234047Z",
     "shell.execute_reply": "2021-12-08T18:41:13.234047Z"
    },
    "papermill": {
     "duration": 0.05201,
     "end_time": "2021-12-08T18:41:13.235044",
     "exception": false,
     "start_time": "2021-12-08T18:41:13.183034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sunshine_drug_integration = pd.read_csv('Data/Outputs_Cleanup/Sunshine_dedupe_drug_integration/data_matching_output.csv')\n",
    "sunshine_druge_distinct = pd.read_csv('Data/Outputs_Cleanup/Sunshine/sunshine_drugs_distinct.csv')\n",
    "\n",
    "sunshine_drug = sunshine_drug_integration[sunshine_drug_integration['source file']==1][['Cluster ID','brand_name']]\n",
    "fda_medD_drug = sunshine_drug_integration[sunshine_drug_integration['source file']==0]\n",
    "\n",
    "fda_medD_drug = fda_medD_drug.merge(fda_partD_drugs[['brand_name','fda_drug_id','MedD_drug_id']],'left','brand_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9919f-0e22-4fdd-8018-b27ee91da205",
   "metadata": {
    "papermill": {
     "duration": 0.011004,
     "end_time": "2021-12-08T18:41:13.262092",
     "exception": false,
     "start_time": "2021-12-08T18:41:13.251088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating Final Drug Lookup Table is Unique By Drug\n",
    "1. `drug_id` (int): Unique drug identifier (generated interally)\n",
    "2. `brand_name` (str): The human readable name of the drug\n",
    "3. `fda_drug_id` (list): The list of OpenFDA drug id's \n",
    "4. `MedD_drug_id` (list): The list of Medicare Part D drug id's \n",
    "5. `sunshine_drug_id` (list): The list of Sunshine Act drug id's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a6257fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T18:41:13.295078Z",
     "iopub.status.busy": "2021-12-08T18:41:13.290061Z",
     "iopub.status.idle": "2021-12-08T18:41:13.331077Z",
     "shell.execute_reply": "2021-12-08T18:41:13.331077Z"
    },
    "papermill": {
     "duration": 0.056999,
     "end_time": "2021-12-08T18:41:13.331077",
     "exception": false,
     "start_time": "2021-12-08T18:41:13.274078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fda_medD_drug = fda_medD_drug[['Cluster ID','brand_name','fda_drug_id','MedD_drug_id']]\n",
    "fda_medD_drug.fillna('left side no match', inplace=True)\n",
    "sunshine_drug.fillna('righ side no match', inplace=True)\n",
    "fda_medD_drug\n",
    "output_drug_matching = fda_medD_drug.merge(sunshine_drug,'outer','Cluster ID')\n",
    "output_drug_matching = output_drug_matching[['brand_name_x','fda_drug_id','MedD_drug_id','brand_name_y']]\n",
    "output_drug_matching.columns = ['brand_name','fda_drug_id','MedD_drug_id','sunshine_drug_id']\n",
    "output_drug_matching.reset_index(inplace=True)\n",
    "output_drug_matching.rename(columns = {'index':'drug_id'},inplace=True)\n",
    "output_drug_matching\n",
    "output_drug_matching['brand_name'] = np.where(\n",
    "    output_drug_matching['brand_name'].isna(),\n",
    "    output_drug_matching.sunshine_drug_id,\n",
    "    output_drug_matching.brand_name\n",
    ")\n",
    "pickle.dump(output_drug_matching, open(\"Data/Outputs_Cleanup/Sunshine_dedupe_drug_integration/final_drug_lookup.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e1d24-ca7f-4f5a-a26d-bce588146a7e",
   "metadata": {
    "papermill": {
     "duration": 0.012009,
     "end_time": "2021-12-08T18:41:13.359085",
     "exception": false,
     "start_time": "2021-12-08T18:41:13.347076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.939462,
   "end_time": "2021-12-08T18:41:13.811170",
   "environment_variables": {},
   "exception": null,
   "input_path": "Final_Drug_Matching.ipynb",
   "output_path": "Logs/2021-12-08Final_Drug_Matching.ipynb",
   "parameters": {
    "retrain": false
   },
   "start_time": "2021-12-08T18:41:00.871708",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c394541-0064-4d1a-b26c-653a9fb09c0a",
   "metadata": {},
   "source": [
    "# OpenFDA Manufacturer Deduplication\n",
    "## Authors: \n",
    "    1. Lam Ho\n",
    "    2. Jonah Breslow\n",
    "    3. Jeffrey Kagan\n",
    "## Purpose:\n",
    "The purpose of this notebook is to internally deduplicate the manufacturer data from the OpenFDA data. The output will be used to match with the Sunshine Act Data that contains manufacturer node information. For this procedure, we utilized the Dedupe.io python implementation.\n",
    "\n",
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddf86bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This code demonstrates how to use dedupe with a comma separated values\n",
    "(CSV) file. All operations are performed in memory, so will run very\n",
    "quickly on datasets up to ~10,000 rows.\n",
    "\n",
    "We start with a CSV file containing our messy data. In this example,\n",
    "it is listings of early childhood education centers in Chicago\n",
    "compiled from several different sources.\n",
    "\n",
    "The output will be a CSV with our clustered results.\n",
    "\n",
    "For larger datasets, see our [mysql_example](mysql_example.html)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import optparse\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e5755-d530-4c78-80ff-69f0eb53ca36",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be82765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openfda = pd.read_csv('../Data/Outputs_Cleanup/FDA/openfda_processed.csv')\n",
    "openfda_manuf = openfda[['manufacturer_name_normalized', 'fda_manuf_id']].drop_duplicates()\n",
    "openfda_manuf.columns = ['manufacturer_name', 'fda_manuf_id']\n",
    "openfda_manuf.to_csv('../Data/Outputs_Cleanup/FDA/openfda_manufacturer_dedupe_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44cff2-7188-4ba5-92a7-bc013ed5d8b9",
   "metadata": {},
   "source": [
    "### Running the Dedupe procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d078403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    # If data is missing, indicate that by setting the value to `None`\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "\n",
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "            row_id = int(row['fda_manuf_id'])\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca63e467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to add on to your training (y/n). If you wanted to start over, delete your .json file \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n",
      "reading from ../Data/Outputs_Cleanup/FDA/csv_example_learned_settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:Predicate set:\n",
      "INFO:dedupe.api:SimplePredicate: (firstTokenPredicate, manufacturer_name)\n",
      "INFO:dedupe.api:(SimplePredicate: (sameSevenCharStartPredicate, manufacturer_name), TfidfNGramCanopyPredicate: (0.4, manufacturer_name))\n",
      "INFO:dedupe.api:(LevenshteinCanopyPredicate: (1, manufacturer_name), TfidfTextCanopyPredicate: (0.2, manufacturer_name), SimplePredicate: (sortedAcronym, manufacturer_name))\n",
      "INFO:dedupe.api:(TfidfNGramCanopyPredicate: (0.6, manufacturer_name), SimplePredicate: (oneGramFingerprint, manufacturer_name), SimplePredicate: (suffixArray, manufacturer_name))\n",
      "INFO:dedupe.api:(SimplePredicate: (firstIntegerPredicate, manufacturer_name), SimplePredicate: (commonSixGram, manufacturer_name), SimplePredicate: (tokenFieldPredicate, manufacturer_name))\n",
      "INFO:dedupe.api:(TfidfNGramCanopyPredicate: (0.8, manufacturer_name), TfidfNGramCanopyPredicate: (0.2, manufacturer_name), TfidfTextCanopyPredicate: (0.4, manufacturer_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.canopy_index:Removing stop word  l\n",
      "INFO:dedupe.canopy_index:Removing stop word e \n",
      "INFO:dedupe.canopy_index:Removing stop word is\n",
      "INFO:dedupe.canopy_index:Removing stop word td\n",
      "INFO:dedupe.canopy_index:Removing stop word  i\n",
      "INFO:dedupe.canopy_index:Removing stop word al\n",
      "INFO:dedupe.canopy_index:Removing stop word at\n",
      "INFO:dedupe.canopy_index:Removing stop word ca\n",
      "INFO:dedupe.canopy_index:Removing stop word ha\n",
      "INFO:dedupe.canopy_index:Removing stop word in\n",
      "INFO:dedupe.canopy_index:Removing stop word ma\n",
      "INFO:dedupe.canopy_index:Removing stop word o \n",
      "INFO:dedupe.canopy_index:Removing stop word rm\n",
      "INFO:dedupe.canopy_index:Removing stop word ti\n",
      "INFO:dedupe.canopy_index:Removing stop word ut\n",
      "INFO:dedupe.canopy_index:Removing stop word io\n",
      "INFO:dedupe.canopy_index:Removing stop word lc\n",
      "INFO:dedupe.canopy_index:Removing stop word n \n",
      "INFO:dedupe.canopy_index:Removing stop word pr\n",
      "INFO:dedupe.canopy_index:Removing stop word s \n",
      "INFO:dedupe.canopy_index:Removing stop word st\n",
      "INFO:dedupe.canopy_index:Removing stop word  c\n",
      "INFO:dedupe.canopy_index:Removing stop word a \n",
      "INFO:dedupe.canopy_index:Removing stop word ra\n",
      "INFO:dedupe.canopy_index:Removing stop word co\n",
      "INFO:dedupe.canopy_index:Removing stop word an\n",
      "INFO:dedupe.canopy_index:Removing stop word la\n",
      "INFO:dedupe.canopy_index:Removing stop word es\n",
      "INFO:dedupe.canopy_index:Removing stop word ea\n",
      "INFO:dedupe.canopy_index:Removing stop word he\n",
      "INFO:dedupe.canopy_index:Removing stop word me\n",
      "INFO:dedupe.canopy_index:Removing stop word re\n",
      "INFO:dedupe.canopy_index:Removing stop word it\n",
      "INFO:dedupe.canopy_index:Removing stop word ch\n",
      "INFO:dedupe.canopy_index:Removing stop word d \n",
      "INFO:dedupe.canopy_index:Removing stop word li\n",
      "INFO:dedupe.canopy_index:Removing stop word or\n",
      "INFO:dedupe.canopy_index:Removing stop word di\n",
      "INFO:dedupe.canopy_index:Removing stop word t \n",
      "INFO:dedupe.canopy_index:Removing stop word ng\n",
      "INFO:dedupe.canopy_index:Removing stop word y \n",
      "INFO:dedupe.canopy_index:Removing stop word ri\n",
      "INFO:dedupe.canopy_index:Removing stop word g \n",
      "INFO:dedupe.canopy_index:Removing stop word ltd\n",
      "INFO:dedupe.canopy_index:Removing stop word inc\n",
      "INFO:dedupe.canopy_index:Removing stop word llc\n",
      "INFO:dedupe.canopy_index:Removing stop word co\n",
      "INFO:dedupe.blocking:10000, 11.0899942 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# duplicate sets 9779\n"
     ]
    }
   ],
   "source": [
    "# ## Setup\n",
    "retrain = input('Do you want to add on to your training (y/n). If you wanted to start over, delete your .json file')\n",
    "isretrain = True if retrain == 'y' else False\n",
    "\n",
    "if isretrain == True:\n",
    "    try:\n",
    "        os.remove('csv_example_learned_settings')\n",
    "    except:\n",
    "        print('Your settings file appears to not have existed.')\n",
    "\n",
    "#input_file = 'csv_example_messy_input.csv'\n",
    "input_file = '../Data/Outputs_Cleanup/FDA/openfda_manufacturer_dedupe_input.csv'\n",
    "output_file = '../Data/Outputs_Cleanup/FDA/openfda_manuf_deduplicated.csv'\n",
    "settings_file = '../Data/Outputs_Cleanup/FDA/csv_example_learned_settings'\n",
    "training_file = '../Data/Outputs_Cleanup/FDA/csv_example_training.json'\n",
    "\n",
    "print('importing data ...')\n",
    "data_d = readData(input_file)\n",
    "\n",
    "# If a settings file already exists, we'll just load that and skip training\n",
    "if os.path.exists(settings_file):\n",
    "    print('reading from', settings_file)\n",
    "    with open(settings_file, 'rb') as f:\n",
    "        deduper = dedupe.StaticDedupe(f)\n",
    "else:\n",
    "    # ## Training\n",
    "\n",
    "    # Define the fields dedupe will pay attention to\n",
    "    fields = [\n",
    "        {'field': 'manufacturer_name', 'type': 'String'},\n",
    "        #{'field': 'Address', 'type': 'String'},\n",
    "        #{'field': 'Zip', 'type': 'Exact', 'has missing': True},\n",
    "        #{'field': 'Phone', 'type': 'String', 'has missing': True},\n",
    "        ]\n",
    "\n",
    "    # Create a new deduper object and pass our data model to it.\n",
    "    deduper = dedupe.Dedupe(fields)\n",
    "\n",
    "    # If we have training data saved from a previous run of dedupe,\n",
    "    # look for it and load it in.\n",
    "    # __Note:__ if you want to train from scratch, delete the training_file\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file, 'rb') as f:\n",
    "            deduper.prepare_training(data_d, f)\n",
    "    else:\n",
    "        deduper.prepare_training(data_d)\n",
    "\n",
    "    # ## Active learning\n",
    "    # Dedupe will find the next pair of records\n",
    "    # it is least certain about and ask you to label them as duplicates\n",
    "    # or not.\n",
    "    # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "    # press 'f' when you are finished\n",
    "    print('starting active labeling...')\n",
    "\n",
    "    dedupe.console_label(deduper)\n",
    "\n",
    "    # Using the examples we just labeled, train the deduper and learn\n",
    "    # blocking predicates\n",
    "    deduper.train()\n",
    "\n",
    "    # When finished, save our training to disk\n",
    "    with open(training_file, 'w') as tf:\n",
    "        deduper.write_training(tf)\n",
    "\n",
    "    # Save our weights and predicates to disk.  If the settings file\n",
    "    # exists, we will skip all the training and learning next time we run\n",
    "    # this file.\n",
    "    with open(settings_file, 'wb') as sf:\n",
    "        deduper.write_settings(sf)\n",
    "\n",
    "# ## Clustering\n",
    "\n",
    "# `partition` will return sets of records that dedupe\n",
    "# believes are all referring to the same entity.\n",
    "\n",
    "print('clustering...')\n",
    "clustered_dupes = deduper.partition(data_d, 0.5)\n",
    "\n",
    "print('# duplicate sets', len(clustered_dupes))\n",
    "\n",
    "# ## Writing Results\n",
    "\n",
    "# Write our original data back out to a CSV with a new column called\n",
    "# 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "cluster_membership = {}\n",
    "for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "    for record_id, score in zip(records, scores):\n",
    "        cluster_membership[record_id] = {\n",
    "            \"Cluster ID\": cluster_id,\n",
    "            \"confidence_score\": score\n",
    "        }\n",
    "\n",
    "with open(output_file, 'w') as f_output, open(input_file) as f_input:\n",
    "\n",
    "    reader = csv.DictReader(f_input)\n",
    "    fieldnames = ['Cluster ID', 'confidence_score'] + reader.fieldnames\n",
    "\n",
    "    writer = csv.DictWriter(f_output, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        row_id = int(row['fda_manuf_id'])\n",
    "        row.update(cluster_membership[row_id])\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b8c19-5d05-4131-ba63-bcc09cb828f7",
   "metadata": {},
   "source": [
    "### Post-Processing the Manufacturer Matches\n",
    "When we cluster multiple manufacturers into the same manufacturer, we create a function that picks the longest `manufacturer_name` to be the cluster name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66977d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_brand(brands):\n",
    "    pick = None\n",
    "    pick_len = 0\n",
    "    for idx, brand in enumerate(brands):\n",
    "        if idx == 0:\n",
    "            pick = brand\n",
    "            pick_len = len(brand)\n",
    "            continue\n",
    "        if len(brand) > pick_len:\n",
    "            pick = brand\n",
    "            pick_len = len(brand)\n",
    "    return pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e702779",
   "metadata": {},
   "outputs": [],
   "source": [
    "openfda_dedupe = pd.read_csv('../Data/Outputs_Cleanup/FDA/openfda_manuf_deduplicated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152eba1-27ba-412e-8e0f-4612c1145795",
   "metadata": {},
   "source": [
    "### Exporting Data to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "928eced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.DataFrame(openfda_dedupe.groupby('Cluster ID')['manufacturer_name'].apply(list))\n",
    "df_cluster['fda_manuf_id'] = openfda_dedupe.groupby('Cluster ID')['fda_manuf_id'].apply(list)\n",
    "df_cluster['manuf_name_picked'] = df_cluster['manufacturer_name'].apply(pick_brand)\n",
    "df_cluster.columns = ['manuf_names', 'fda_manuf_id', 'manufacturer_name']\n",
    "pickle.dump(df_cluster, open( \"../Data/Outputs_Cleanup/FDA/openfda_manufacturer_deduplicated_single_manuf.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a088af-e847-45b2-8090-4fa790cb7436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
